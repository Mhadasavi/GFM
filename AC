Module 1
1. Get Metadata from files and store in csv file.
    Done

2. Implement elapsed time and input of directory from user.
    Done

3. Adding New Path removes the old content from the csv file. Update the logic to keep the old content also.
    keep historical data and do incremental updates:
        * During each run, create a new metadata CSV (e.g., file_metadata_img_2025.csv, file_metadata_videos_2025.csv)
        * Create a master CSV (e.g., file_metadata_master.csv)
            * Use a script to merge the new one into the master, avoiding duplicates.
            * Merging strategy: (Optional, currently merging based on filename)
                - Consider (file_path + modified_date) as the unique key to deduplicate entries.
                - Or hash the file_path + size_bytes + modified_date as a dedup key.
    Done

Optional
1. Implement meta_row_id, log start and end time as elapsed time is not accurate.
2. Use Sql Db to dump all csvs into tables.
3. Add pre-commit hook and requirement.txt
    Done
4. Dont Append header if already exist during 2nd run
    Done
5. Throw file already open exception if file opens during writing.
    Done

Module 2
1. Establish connection from GDrive.
2. Get Metadata of drive's files and store in a new csv file.
3. Implement step 2 & 3 of module 1.
4. Compare both the csvs and add the duplicates into the new csv file.
5. Delete the duplicates from drive.
6. Visulalize the data.

Module 3
1. Create a data pipeline with following tools -
    - Pyspark
    - Delta tables
    - Airflow
    - and other required modern tools.

Note - Test Module 1 & 2 on limited files and trigger the flow once module 3 completed. So that we can leverage everything and
monitor everything.